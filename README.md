# Welcome to Creative Writing Benchmark

## Project info

**URL**: https://lovable.dev/projects/REPLACE_WITH_PROJECT_ID

## ğŸ“˜ Creative Writing Benchmark

LLM Evaluation Platform for Creative Writing Quality
A research-grade web application for benchmarking large language models (LLMs) on creative writing tasks, using multi-metric evaluation and LLM-as-a-judge scoring.


## ğŸ” Overview
### Creative Writing Benchmark allows users to:
. Compare multiple LLMs on the same creative prompt
. Automatically evaluate outputs using a judge model
### . Score models on:
    . Theme Coherence
    . Creativity
    . Fluency
    . Engagement
. Visualize results through charts and leaderboards
. Run in Demo Mode (no API credits required) or Live Mode (OpenRouter / LiteLLM)

### This project is suitable for:
. Research demos
. Model comparison studies
. Product evaluation
. Client presentations


## Key Features
. Multi-model benchmarking (OpenAI, OpenRouter, Anthropic, Moonshot, etc.)
. LLM-as-a-Judge evaluation pipeline
. Multi-metric scoring (0â€“40 total score)
. Visual analytics (charts & leaderboard)
. Demo / Preview mode (no API billing required)
. Deployed on Render (production-ready)

### Screenshots
Add screenshots to the /screenshots folder and reference them below.
Benchmark Configuration
Results & Charts
Leaderboard
âš™ï¸ Architecture
Frontend (React + Vite)
        |
        | POST /api/benchmark
        â†“
Node.js Server (Express)
        |
        | OpenAI-compatible API
        â†“
LiteLLM / OpenRouter
ğŸ”‘ Environment Variables
Configure the following in Render â†’ Environment or a local .env file.
Required (Live Mode)
LITELLM_BASE_URL=https://openrouter.ai/api/v1
LITELLM_API_KEY=sk-or-xxxxxxxxxxxxxxxx
Optional (Demo Mode)
MOCK_MODE=true
When MOCK_MODE=true, the app renders charts and scores without calling external APIs â€” perfect for demos when credits are unavailable.
ğŸš€ Running Locally
npm install
npm run build
npm start
App will run on:
http://localhost:10000
ğŸŒ Production Deployment
Hosted on Render
Auto-deploys on main branch push
Uses Node.js + static Vite build
To redeploy:
git push origin main
ğŸ§ª Demo vs Live Mode
Mode	Description
Demo Mode	Visual demo with mock scores (no billing)
Live Mode	Real API calls using OpenRouter / LiteLLM
Switch modes via environment variables â€” no code changes required.
ğŸ“Š Evaluation Metrics
Each model output is scored on:
Metric	Range
Theme Coherence	0â€“10
Creativity	0â€“10
Fluency	0â€“10
Engagement	0â€“10
Total Score	0â€“40
ğŸ§¾ API Endpoint
POST /api/benchmark
Request
{
  "prompt": "Write a short story about a lighthouse keeper...",
  "models": ["openai/gpt-4o-mini"],
  "judgeModel": "openai/gpt-4o-mini"
}
Response
{
  "results": [
    {
      "modelName": "gpt-4o-mini",
      "provider": "openai",
      "themeCoherence": 8,
      "creativity": 7,
      "fluency": 9,
      "engagement": 8,
      "totalScore": 32
    }
  ]
}
ğŸ“Œ Notes for Client
If results do not appear:
Check API credits
Verify LITELLM_API_KEY
Use Demo Mode for preview
Model availability depends on OpenRouter account permissions
ğŸ‘¤ Author
Creative Writing Benchmark
Developed by Labib
Research & AI Engineering
